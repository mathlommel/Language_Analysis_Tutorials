{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9i31IywsUHPb"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Delivery nÂ°3 - Mathias Lommel**"
      ],
      "metadata": {
        "id": "Z4nAndHVNiQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *1 - Libraries Importation*"
      ],
      "metadata": {
        "id": "9i31IywsUHPb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "cm6xsQh7wSi2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f0633d-fcc2-49cc-c22f-2b53b55dfd9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# All the necessary imports\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "import sklearn\n",
        "import sklearn.feature_extraction\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Importation of SpaCy\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *2 - Definition of the functions*"
      ],
      "metadata": {
        "id": "7aQs6zQZUN49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I couldn't download the language library on my own computer, then, I have done this third delivery on Google Colab.\n",
        "# As a consequence, I have computed 2 different ways to read the input document :\n",
        "##     one using Colab (the document being on \"My Drive\")\n",
        "##     another one, which can be used if the document is directly on the computer.\n",
        "\n",
        "def read_document_with_Drive():\n",
        "  # Read the document on Drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  # Reading of the .txt document via Colab\n",
        "  path = '/content/drive/My Drive/Romeo_and_Juliet.txt'\n",
        "\n",
        "  # Creation of the corpus\n",
        "  original_corpus=[]\n",
        "  with open(path, 'r', errors = 'ignore') as f:\n",
        "    # We add each line to the corpus\n",
        "    for line in f:\n",
        "      original_corpus.append(line.strip())\n",
        "\n",
        "  print(\"Document has been read successfully.\")\n",
        "  print(\"This corpus contains\", len(original_corpus), \"documents.\")\n",
        "\n",
        "  return original_corpus\n",
        "\n",
        "def read_document():\n",
        "  # Read the document directly on the computer\n",
        "  with open(\"Romeo_and_Juliet.txt\", \"r\") as file:\n",
        "      # We create the corpus\n",
        "      original_corpus = file.readlines()\n",
        "\n",
        "  print(\"Document has been read successfully.\")\n",
        "  print(\"This corpus contains\", len(original_corpus), \"documents.\")\n",
        "  return original_corpus"
      ],
      "metadata": {
        "id": "3cjSq2DAOTM1"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that cleans the corpus\n",
        "def preprocess_corpus(original_corpus):\n",
        "  cleaned_corpus = []\n",
        "\n",
        "  k=0\n",
        "  # We clean each document of the corpus\n",
        "  for document in original_corpus:\n",
        "    # Change to lower case\n",
        "    document = document.lower()\n",
        "\n",
        "    # Remove URLs (http and https)\n",
        "    document = re.sub(\"http?:\\/\\/.*[\\r\\n]*\", \"\", document)\n",
        "    document = re.sub(\"https?:\\/\\/.*[\\r\\n]*\", \"\", document)\n",
        "\n",
        "    # Remove emails\n",
        "    document= re.sub(r'\\b[A-Za-z0-9._-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b','',document)\n",
        "\n",
        "    # Remove mentions\n",
        "    document = re.sub(\"@\\S+\", \"\", document)\n",
        "\n",
        "    # Remove punctuations, commas and special characters\n",
        "    punctuation = string.punctuation\n",
        "    translation_table = str.maketrans('', '', punctuation)\n",
        "\n",
        "    document = document.translate(translation_table)\n",
        "\n",
        "    # Remove numbers\n",
        "    document = re.sub(r'\\d+', '', document)\n",
        "\n",
        "    # Remove the \\n\n",
        "    document = re.sub('\\\\n', '', document)\n",
        "\n",
        "    if (not document.isspace()) and document != '':\n",
        "        # If the tweet is still interesting\n",
        "        cleaned_corpus.append(document)\n",
        "    else:\n",
        "        # If not, we delete it from the original corpus\n",
        "        original_corpus.pop(k)\n",
        "\n",
        "    k += 1\n",
        "\n",
        "  print(\"Pre-processing succesfully computed.\")\n",
        "  print(\"Length of the initial data : \",len(original_corpus))\n",
        "  print(\"Length of the cleaned data : \",len(cleaned_corpus))\n",
        "\n",
        "  return (original_corpus, cleaned_corpus)"
      ],
      "metadata": {
        "id": "0nnxHPq-SxBO"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that computes the tokens of the corpus\n",
        "def get_tokens (corpus):\n",
        "  tokens = []\n",
        "  # For each document of the corpus\n",
        "  for document in corpus:\n",
        "    doc_tokens = nlp(document)\n",
        "\n",
        "    for token in doc_tokens:\n",
        "      # We add the token if it's not already done\n",
        "      if not str(token) in str(tokens):\n",
        "        tokens.append(token)\n",
        "\n",
        "  print(\"The corpus contains\",len(tokens),\"different tokens.\")\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "n9quQJ3twlCG"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that removes the stop words\n",
        "def remove_stopWords (tokens,stop_words=''):\n",
        "  if stop_words == '':\n",
        "    # 2 options :\n",
        "    ##  - give the stop_words that we want to remove\n",
        "    ##  - remove the default stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  tokens_sw = [token for token in tokens if str(token) not in stop_words]\n",
        "\n",
        "  print(\"Before removing stop words :\",len(tokens), \"words\")\n",
        "  print(\"After removing stop words :\",len(tokens_sw), \"words\")\n",
        "\n",
        "  return tokens_sw"
      ],
      "metadata": {
        "id": "SEdS97fSWU28"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that determines the proper nouns of the corpus\n",
        "def get_properNames (tokens):\n",
        "  proper_names = []\n",
        "  # We browse each token, and see if it's a PROPN\n",
        "  for token in tokens:\n",
        "    if str(token.pos_) == \"PROPN\":\n",
        "      proper_names.append(token)\n",
        "\n",
        "  return proper_names"
      ],
      "metadata": {
        "id": "24sDf9eBZLx8"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that determines the nouns of the corpus\n",
        "def get_Nouns (tokens):\n",
        "  nouns = []\n",
        "  # We browse each token, and see if it's a NOUN\n",
        "  for token in tokens:\n",
        "    if str(token.pos_) == \"NOUN\":\n",
        "      nouns.append(token)\n",
        "\n",
        "  print(\"The corpus contains\",len(nouns),\"nouns.\")\n",
        "  return nouns"
      ],
      "metadata": {
        "id": "4FMLOjZsrrWL"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To get the Document Term Matrix\n",
        "def get_dtm(corpus):\n",
        "    vec = CountVectorizer()\n",
        "    X = vec.fit_transform(corpus)\n",
        "    # Creation of the matrix\n",
        "    term_matrix = pd.DataFrame(X.toarray(), columns = vec.get_feature_names_out())\n",
        "\n",
        "    return term_matrix"
      ],
      "metadata": {
        "id": "vpbh6qjTtE3B"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To get the most repeated nouns of the corpus\n",
        "def get_mostRepeated_noun(corpus, tokens):\n",
        "  # We get the document term matrix\n",
        "  term_matrix = get_dtm(corpus)\n",
        "\n",
        "  # We get all the nouns of the corpus\n",
        "  nouns = get_Nouns(tokens)\n",
        "\n",
        "  # We compute the occurences of each word of the corpus, and we sort it\n",
        "  words_occurences = term_matrix.sum(axis = 1)\n",
        "  sorted = words_occurences.sort_values(ascending=False)\n",
        "  words = term_matrix.columns\n",
        "\n",
        "  # We initialize the list (result of the function)\n",
        "  mostRepeated = []\n",
        "  nb_occ = 0\n",
        "\n",
        "  for i in sorted.index:\n",
        "    # If the word is a noun\n",
        "    if words[i] in str(nouns):\n",
        "      # If the noun is one of the most repeated\n",
        "      if sorted[i] >= nb_occ:\n",
        "        # We add it to the list\n",
        "        mostRepeated.append([words[i], sorted[i]])\n",
        "        # We change nb_occ (useful for the first iteration)\n",
        "        nb_occ = sorted[i]\n",
        "      else:\n",
        "        # If not, we have seen all most repeated nouns : we return the list\n",
        "        return mostRepeated"
      ],
      "metadata": {
        "id": "CFUgn0cVtvVY"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To get the frequencies of each root word of the corpus\n",
        "# 1st VERSION : considers all the tokens (tokens can be repeated)\n",
        "def root_words(corpus):\n",
        "  # We initialize a dictionary to get the frequencies of each root word\n",
        "  token_root = {}\n",
        "  s_stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "  # We browse all documents\n",
        "  for document in corpus:\n",
        "    doc_tokens = nlp(document)\n",
        "    # For each token of the document\n",
        "    for token in doc_tokens:\n",
        "      root_word = s_stemmer.stem(str(token))\n",
        "\n",
        "      # We add 1 to the frequency of the token's root\n",
        "      if str(root_word) in token_root:\n",
        "        token_root[str(root_word)] += 1\n",
        "      else:\n",
        "        token_root[str(root_word)] = 1\n",
        "\n",
        "  return token_root"
      ],
      "metadata": {
        "id": "8vlCRiA4eFwd"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To get the n more used root words of the corpus (1st VERSION)\n",
        "def more_frequent_roots(corpus, n):\n",
        "  # We get the frequencies of each root word\n",
        "  token_root = root_words(corpus)\n",
        "  # We determine the n more sed words\n",
        "  root_df = pd.DataFrame(token_root.values(), index = token_root.keys())\n",
        "  root_freq = root_df.nlargest(15,root_df.columns)\n",
        "\n",
        "  return root_freq\n"
      ],
      "metadata": {
        "id": "bc9CWhLKMTIU"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To get the frequencies of each root word of the corpus\n",
        "# 2nd VERSION : the tokens are not repeated\n",
        "def root_words2(tokens):\n",
        "  # We initialize a dictionary to get the frequencies of each root word\n",
        "  token_root = {}\n",
        "  s_stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "  # We browse all documents\n",
        "  for token in tokens:\n",
        "    root_word = s_stemmer.stem(str(token))\n",
        "\n",
        "    # We add 1 to the frequency of the token's root\n",
        "    if str(root_word) in token_root:\n",
        "      token_root[str(root_word)] += 1\n",
        "    else:\n",
        "      token_root[str(root_word)] = 1\n",
        "\n",
        "  return token_root"
      ],
      "metadata": {
        "id": "Hxv5OEe0SuiW"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To get the n more used root words of the corpus (2nd VERSION)\n",
        "def more_frequent_roots2(tokens, n):\n",
        "  # We get the frequencies of each root word\n",
        "  token_root = root_words2(tokens)\n",
        "  # We determine the n more sed words\n",
        "  root_df = pd.DataFrame(token_root.values(), index = token_root.keys())\n",
        "  root_freq = root_df.nlargest(15,root_df.columns)\n",
        "\n",
        "  return root_freq"
      ],
      "metadata": {
        "id": "FhSU2AyuTGx2"
      },
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *3 - Test of our functions*"
      ],
      "metadata": {
        "id": "IKGl6L1jN7f9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We create the corpus\n",
        "print(\"*********************** Pre-processing *************************\")\n",
        "original_corpus = read_document_with_Drive()\n",
        "# OR\n",
        "#original_corpus = read_document()\n",
        "\n",
        "# We pre-process the corpus\n",
        "Data = preprocess_corpus(original_corpus)\n",
        "original_corpus = Data[0]\n",
        "cleaned_corpus = Data[1]\n",
        "print(\"****************************************************************\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-tJMfvBO5DM",
        "outputId": "681952ac-07a3-46b8-9218-20dce2f7d188"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*********************** Pre-processing *************************\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Document has been read successfully.\n",
            "This corpus contains 364 documents.\n",
            "Pre-processing succesfully computed.\n",
            "Length of the initial data :  352\n",
            "Length of the cleaned data :  340\n",
            "****************************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the tokens of the corpus\n",
        "print(\"************************ Tokenization **************************\")\n",
        "tokens = get_tokens(cleaned_corpus)\n",
        "\n",
        "# Remove the stop words : we remove the default stop words list\n",
        "tokens_sw = remove_stopWords(tokens)\n",
        "print(\"We can see that the number of tokens has changed.\")\n",
        "print(\"****************************************************************\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u10wZEAEUbdU",
        "outputId": "e42089b7-982e-4c32-b311-711a2e0c5c22"
      },
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************ Tokenization **************************\n",
            "The corpus contains 584 different tokens.\n",
            "Before removing stop words : 584 words\n",
            "After removing stop words : 522 words\n",
            "We can see that the number of tokens has changed.\n",
            "****************************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"************************ Knowledge base ************************\")\n",
        "for token in tokens_sw:\n",
        "  print(token.text, '\\t', token.pos_, '\\t', token.lemma_)\n",
        "print(\"****************************************************************\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Kid8Re9Csgc",
        "outputId": "eb49bbb7-0caa-4dcd-9a16-a23ea0a9e429"
      },
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************ Knowledge base ************************\n",
            "gregory \t NOUN \t gregory\n",
            "shows \t VERB \t show\n",
            "thee \t PRON \t thee\n",
            "weak \t ADJ \t weak\n",
            "slave \t NOUN \t slave\n",
            "weakest \t ADJ \t weak\n",
            "goes \t VERB \t go\n",
            "wall \t NOUN \t wall\n",
            "sampson \t NOUN \t sampson\n",
            "true \t ADJ \t true\n",
            "therefore \t ADV \t therefore\n",
            "women \t NOUN \t woman\n",
            "weaker \t ADJ \t weak\n",
            "vessels \t NOUN \t vessel\n",
            "ever \t ADV \t ever\n",
            "thrust \t VERB \t thrust\n",
            "push \t VERB \t push\n",
            "montagues \t VERB \t montague\n",
            "maids \t NOUN \t maid\n",
            "quarrel \t NOUN \t quarrel\n",
            "masters \t NOUN \t master\n",
            "tis \t VERB \t tis\n",
            "one \t NOUN \t one\n",
            "tyrant \t NOUN \t tyrant\n",
            "fought \t VERB \t fight\n",
            "cruel \t ADJ \t cruel\n",
            "cut \t VERB \t cut\n",
            "heads \t NOUN \t head\n",
            "ay \t VERB \t ay\n",
            "maidenheads \t NOUN \t maidenhead\n",
            "take \t VERB \t take\n",
            "sense \t NOUN \t sense\n",
            "thou \t NOUN \t thou\n",
            "wilt \t NOUN \t wilt\n",
            "must \t AUX \t must\n",
            "feel \t VERB \t feel\n",
            "shall \t AUX \t shall\n",
            "able \t ADJ \t able\n",
            "stand \t VERB \t stand\n",
            "known \t VERB \t know\n",
            "pretty \t ADJ \t pretty\n",
            "piece \t NOUN \t piece\n",
            "esh \t NOUN \t esh\n",
            "well \t INTJ \t well\n",
            "art \t NOUN \t art\n",
            "hadst \t PROPN \t hadst\n",
            "romeo \t PROPN \t romeo\n",
            "juliet \t PROPN \t juliet\n",
            "act \t NOUN \t act\n",
            "scene \t NOUN \t scene\n",
            "william \t PROPN \t william\n",
            "shakespeare \t PROPN \t shakespeare\n",
            "poor \t ADJ \t poor\n",
            "john \t PROPN \t john\n",
            "draw \t VERB \t draw\n",
            "thy \t PRON \t thy\n",
            "tool \t NOUN \t tool\n",
            "comes \t VERB \t come\n",
            "two \t NUM \t two\n",
            "house \t NOUN \t house\n",
            "naked \t ADJ \t naked\n",
            "weapon \t NOUN \t weapon\n",
            "back \t VERB \t back\n",
            "turn \t VERB \t turn\n",
            "run \t VERB \t run\n",
            "fear \t VERB \t fear\n",
            "marry \t NOUN \t marry\n",
            "let \t VERB \t let\n",
            "law \t NOUN \t law\n",
            "sides \t NOUN \t side\n",
            "begin \t VERB \t begin\n",
            "frown \t VERB \t frown\n",
            "pass \t VERB \t pass\n",
            "list \t VERB \t list\n",
            "nay \t INTJ \t nay\n",
            "dare \t VERB \t dare\n",
            "bite \t VERB \t bite\n",
            "thumb \t NOUN \t thumb\n",
            "disgrace \t NOUN \t disgrace\n",
            "bear \t VERB \t bear\n",
            "enter \t VERB \t enter\n",
            "abraham \t PROPN \t abraham\n",
            "balthasar \t VERB \t balthasar\n",
            "sir \t PUNCT \t sir\n",
            "aside \t ADV \t aside\n",
            "say \t VERB \t say\n",
            "serve \t VERB \t serve\n",
            "good \t ADJ \t good\n",
            "man \t NOUN \t man\n",
            "better \t ADJ \t well\n",
            "kinsmen \t NOUN \t kinsman\n",
            "yes \t INTJ \t yes\n",
            "remember \t VERB \t remember\n",
            "swashing \t VERB \t swash\n",
            "blow \t NOUN \t blow\n",
            "benvolio \t NOUN \t benvolio\n",
            "part \t NOUN \t part\n",
            "fools \t NOUN \t fool\n",
            "put \t VERB \t put\n",
            "swords \t NOUN \t sword\n",
            "beats \t VERB \t beat\n",
            "tybalt \t NOUN \t tybalt\n",
            "drawn \t VERB \t draw\n",
            "among \t ADP \t among\n",
            "heartless \t ADJ \t heartless\n",
            "hinds \t NOUN \t hind\n",
            "look \t VERB \t look\n",
            "upon \t SCONJ \t upon\n",
            "death \t NOUN \t death\n",
            "keep \t VERB \t keep\n",
            "peace \t NOUN \t peace\n",
            "manage \t VERB \t manage\n",
            "talk \t NOUN \t talk\n",
            "hate \t VERB \t hate\n",
            "hell \t PROPN \t hell\n",
            "coward \t NOUN \t coward\n",
            "several \t ADJ \t several\n",
            "houses \t NOUN \t house\n",
            "join \t VERB \t join\n",
            "fray \t NOUN \t fray\n",
            "citizens \t NOUN \t citizen\n",
            "clubs \t NOUN \t club\n",
            "first \t ADJ \t first\n",
            "bills \t NOUN \t bill\n",
            "partisans \t NOUN \t partisan\n",
            "strike \t VERB \t strike\n",
            "capulets \t NOUN \t capulet\n",
            "gown \t NOUN \t gown\n",
            "lady \t NOUN \t lady\n",
            "noise \t NOUN \t noise\n",
            "give \t VERB \t give\n",
            "long \t ADJ \t long\n",
            "crutch \t NOUN \t crutch\n",
            "call \t VERB \t call\n",
            "old \t ADJ \t old\n",
            "ourishes \t VERB \t ourishe\n",
            "blade \t NOUN \t blade\n",
            "spite \t NOUN \t spite\n",
            "villain \t PROPN \t villain\n",
            "capulethold \t NOUN \t capulethold\n",
            "shalt \t NOUN \t shalt\n",
            "stir \t VERB \t stir\n",
            "foot \t NOUN \t foot\n",
            "seek \t VERB \t seek\n",
            "foe \t NOUN \t foe\n",
            "prince \t NOUN \t prince\n",
            "attendants \t NOUN \t attendant\n",
            "rebellious \t ADJ \t rebellious\n",
            "subjects \t NOUN \t subject\n",
            "enemies \t NOUN \t enemy\n",
            "profaners \t NOUN \t profaner\n",
            "neighbourstained \t VERB \t neighbourstaine\n",
            "steel \t NOUN \t steel\n",
            "beasts \t NOUN \t beast\n",
            "quench \t VERB \t quench\n",
            "pernicious \t ADJ \t pernicious\n",
            "rage \t NOUN \t rage\n",
            "purple \t ADJ \t purple\n",
            "fountains \t NOUN \t fountain\n",
            "issuing \t VERB \t issue\n",
            "veins \t NOUN \t vein\n",
            "pain \t NOUN \t pain\n",
            "torture \t NOUN \t torture\n",
            "bloody \t ADJ \t bloody\n",
            "hands \t NOUN \t hand\n",
            "throw \t VERB \t throw\n",
            "mistemperd \t ADJ \t mistemperd\n",
            "weapons \t NOUN \t weapon\n",
            "ground \t NOUN \t ground\n",
            "sentence \t NOUN \t sentence\n",
            "moved \t ADJ \t moved\n",
            "three \t NUM \t three\n",
            "civil \t ADJ \t civil\n",
            "brawls \t NOUN \t brawl\n",
            "bred \t VERB \t breed\n",
            "airy \t NOUN \t airy\n",
            "thrice \t NOUN \t thrice\n",
            "disturbd \t NOUN \t disturbd\n",
            "quiet \t ADJ \t quiet\n",
            "streets \t NOUN \t street\n",
            "made \t VERB \t make\n",
            "veronas \t ADJ \t veronas\n",
            "ancient \t ADJ \t ancient\n",
            "cast \t VERB \t cast\n",
            "grave \t ADJ \t grave\n",
            "beseeming \t NOUN \t beseeming\n",
            "ornaments \t NOUN \t ornament\n",
            "wield \t VERB \t wield\n",
            "cankerd \t NOUN \t cankerd\n",
            "lives \t NOUN \t life\n",
            "pay \t VERB \t pay\n",
            "forfeit \t NOUN \t forfeit\n",
            "time \t NOUN \t time\n",
            "rest \t NOUN \t rest\n",
            "depart \t NOUN \t depart\n",
            "away \t ADV \t away\n",
            "along \t ADP \t along\n",
            "afternoon \t NOUN \t afternoon\n",
            "pleasure \t NOUN \t pleasure\n",
            "case \t NOUN \t case\n",
            "freetown \t VERB \t freetown\n",
            "common \t ADJ \t common\n",
            "judgmentplace \t NOUN \t judgmentplace\n",
            "exeunt \t VERB \t exeunt\n",
            "set \t VERB \t set\n",
            "new \t ADJ \t new\n",
            "abroach \t NOUN \t abroach\n",
            "speak \t PROPN \t speak\n",
            "nephew \t PROPN \t nephew\n",
            "began \t VERB \t begin\n",
            "servants \t NOUN \t servant\n",
            "adversary \t NOUN \t adversary\n",
            "close \t ADJ \t close\n",
            "ghting \t VERB \t ghte\n",
            "approach \t VERB \t approach\n",
            "drew \t VERB \t draw\n",
            "instant \t NOUN \t instant\n",
            "came \t VERB \t come\n",
            "ery \t NOUN \t ery\n",
            "prepared \t VERB \t prepare\n",
            "breathed \t VERB \t breathe\n",
            "deance \t NOUN \t deance\n",
            "ears \t NOUN \t ear\n",
            "swung \t VERB \t swing\n",
            "winds \t NOUN \t wind\n",
            "nothing \t PRON \t nothing\n",
            "hurt \t VERB \t hurt\n",
            "withal \t NOUN \t withal\n",
            "hissd \t NOUN \t hissd\n",
            "scorn \t NOUN \t scorn\n",
            "interchanging \t VERB \t interchange\n",
            "thrusts \t NOUN \t thrust\n",
            "blows \t VERB \t blow\n",
            "till \t SCONJ \t till\n",
            "parted \t VERB \t part\n",
            "either \t DET \t either\n",
            "saw \t VERB \t see\n",
            "today \t NOUN \t today\n",
            "right \t INTJ \t right\n",
            "glad \t ADJ \t glad\n",
            "madam \t PROPN \t madam\n",
            "hour \t NOUN \t hour\n",
            "worshippd \t PROPN \t worshippd\n",
            "sun \t PROPN \t sun\n",
            "peerd \t NOUN \t peerd\n",
            "forth \t ADP \t forth\n",
            "golden \t ADJ \t golden\n",
            "window \t NOUN \t window\n",
            "troubled \t ADJ \t troubled\n",
            "mind \t NOUN \t mind\n",
            "drave \t VERB \t drave\n",
            "walk \t VERB \t walk\n",
            "abroad \t ADV \t abroad\n",
            "underneath \t ADP \t underneath\n",
            "grove \t NOUN \t grove\n",
            "sycamore \t NOUN \t sycamore\n",
            "westward \t ADV \t westward\n",
            "rooteth \t ADJ \t rooteth\n",
            "citys \t ADJ \t citys\n",
            "early \t ADJ \t early\n",
            "walking \t NOUN \t walking\n",
            "towards \t ADP \t towards\n",
            "ware \t ADJ \t ware\n",
            "stole \t VERB \t steal\n",
            "covert \t NOUN \t covert\n",
            "wood \t NOUN \t wood\n",
            "measuring \t VERB \t measure\n",
            "affections \t NOUN \t affection\n",
            "busied \t VERB \t busy\n",
            "alone \t ADJ \t alone\n",
            "pursued \t VERB \t pursue\n",
            "humour \t NOUN \t humour\n",
            "pursuing \t VERB \t pursue\n",
            "gladly \t ADV \t gladly\n",
            "shunnd \t ADV \t shunnd\n",
            "many \t DET \t many\n",
            "morning \t NOUN \t morning\n",
            "hath \t NOUN \t hath\n",
            "seen \t VERB \t see\n",
            "tears \t NOUN \t tear\n",
            "augmenting \t VERB \t augment\n",
            "fresh \t ADJ \t fresh\n",
            "dew \t NOUN \t dew\n",
            "adding \t VERB \t add\n",
            "clouds \t NOUN \t cloud\n",
            "deep \t ADJ \t deep\n",
            "sighs \t NOUN \t sigh\n",
            "soon \t ADV \t soon\n",
            "allcheering \t VERB \t allcheere\n",
            "furthest \t ADJ \t furth\n",
            "shady \t ADJ \t shady\n",
            "curtains \t NOUN \t curtain\n",
            "auroras \t NOUN \t aurora\n",
            "bed \t NOUN \t bed\n",
            "light \t ADJ \t light\n",
            "steals \t NOUN \t steal\n",
            "home \t ADV \t home\n",
            "heavy \t ADJ \t heavy\n",
            "private \t ADJ \t private\n",
            "chamber \t NOUN \t chamber\n",
            "pens \t NOUN \t pen\n",
            "shuts \t VERB \t shut\n",
            "windows \t NOUN \t window\n",
            "locks \t NOUN \t lock\n",
            "far \t ADV \t far\n",
            "daylight \t NOUN \t daylight\n",
            "makes \t VERB \t make\n",
            "articial \t ADJ \t articial\n",
            "night \t NOUN \t night\n",
            "black \t ADJ \t black\n",
            "portentous \t ADJ \t portentous\n",
            "prove \t VERB \t prove\n",
            "unless \t SCONJ \t unless\n",
            "counsel \t NOUN \t counsel\n",
            "may \t AUX \t may\n",
            "cause \t NOUN \t cause\n",
            "remove \t VERB \t remove\n",
            "noble \t ADJ \t noble\n",
            "uncle \t NOUN \t uncle\n",
            "neither \t CCONJ \t neither\n",
            "learn \t VERB \t learn\n",
            "importuned \t VERB \t importune\n",
            "means \t NOUN \t mean\n",
            "friends \t NOUN \t friend\n",
            "counsellor \t NOUN \t counsellor\n",
            "himselfi \t ADJ \t himselfi\n",
            "secret \t ADJ \t secret\n",
            "sounding \t VERB \t sound\n",
            "discovery \t NOUN \t discovery\n",
            "bud \t NOUN \t bud\n",
            "envious \t ADJ \t envious\n",
            "worm \t NOUN \t worm\n",
            "spread \t VERB \t spread\n",
            "sweet \t ADJ \t sweet\n",
            "leaves \t NOUN \t leave\n",
            "dedicate \t VERB \t dedicate\n",
            "beauty \t NOUN \t beauty\n",
            "could \t AUX \t could\n",
            "whence \t NOUN \t whence\n",
            "sorrows \t NOUN \t sorrow\n",
            "grow \t VERB \t grow\n",
            "would \t AUX \t would\n",
            "willingly \t ADV \t willingly\n",
            "cure \t NOUN \t cure\n",
            "please \t INTJ \t please\n",
            "step \t VERB \t step\n",
            "grievance \t NOUN \t grievance\n",
            "much \t ADV \t much\n",
            "denied \t VERB \t deny\n",
            "wert \t VERB \t wert\n",
            "happy \t ADJ \t happy\n",
            "stay \t NOUN \t stay\n",
            "shrift \t NOUN \t shrift\n",
            "goodmorrow \t PROPN \t goodmorrow\n",
            "cousin \t PROPN \t cousin\n",
            "young \t ADJ \t young\n",
            "struck \t VERB \t strike\n",
            "nine \t NUM \t nine\n",
            "sad \t ADJ \t sad\n",
            "hours \t NOUN \t hour\n",
            "father \t NOUN \t father\n",
            "went \t VERB \t go\n",
            "fast \t ADV \t fast\n",
            "sadness \t NOUN \t sadness\n",
            "lengthens \t VERB \t lengthen\n",
            "romeos \t NOUN \t romeo\n",
            "short \t ADJ \t short\n",
            "love \t NOUN \t love\n",
            "favour \t NOUN \t favour\n",
            "alas \t INTJ \t alas\n",
            "gentle \t ADJ \t gentle\n",
            "view \t NOUN \t view\n",
            "tyrannous \t ADJ \t tyrannous\n",
            "rough \t ADJ \t rough\n",
            "proof \t NOUN \t proof\n",
            "whose \t DET \t whose\n",
            "muffled \t VERB \t muffle\n",
            "still \t ADV \t still\n",
            "without \t ADP \t without\n",
            "eyes \t NOUN \t eye\n",
            "pathways \t NOUN \t pathway\n",
            "dine \t VERB \t dine\n",
            "yet \t ADV \t yet\n",
            "tell \t VERB \t tell\n",
            "heard \t VERB \t hear\n",
            "heres \t VERB \t here\n",
            "brawling \t VERB \t brawl\n",
            "loving \t VERB \t love\n",
            "create \t VERB \t create\n",
            "lightness \t NOUN \t lightness\n",
            "serious \t ADJ \t serious\n",
            "vanity \t NOUN \t vanity\n",
            "misshapen \t PROPN \t misshapen\n",
            "chaos \t NOUN \t chaos\n",
            "wellseeming \t VERB \t wellseeme\n",
            "forms \t NOUN \t form\n",
            "feather \t NOUN \t feather\n",
            "lead \t ADJ \t lead\n",
            "bright \t ADJ \t bright\n",
            "smoke \t NOUN \t smoke\n",
            "cold \t ADJ \t cold\n",
            "fire \t NOUN \t fire\n",
            "sick \t ADJ \t sick\n",
            "health \t NOUN \t health\n",
            "stillwaking \t NOUN \t stillwaking\n",
            "sleep \t NOUN \t sleep\n",
            "dost \t X \t dost\n",
            "laugh \t ADJ \t laugh\n",
            "coz \t NOUN \t coz\n",
            "rather \t ADV \t rather\n",
            "weep \t VERB \t weep\n",
            "hearts \t NOUN \t heart\n",
            "oppression \t NOUN \t oppression\n",
            "loves \t VERB \t love\n",
            "transgression \t NOUN \t transgression\n",
            "griefs \t NOUN \t grief\n",
            "mine \t PRON \t mine\n",
            "breast \t NOUN \t breast\n",
            "propagate \t VERB \t propagate\n",
            "prest \t ADJ \t pr\n",
            "thine \t NOUN \t thine\n",
            "hast \t PROPN \t hast\n",
            "shown \t VERB \t show\n",
            "doth \t NOUN \t doth\n",
            "raised \t VERB \t raise\n",
            "fume \t NOUN \t fume\n",
            "purged \t VERB \t purge\n",
            "sparkling \t VERB \t sparkle\n",
            "lovers \t NOUN \t lover\n",
            "vexd \t VERB \t vexd\n",
            "sea \t NOUN \t sea\n",
            "nourishd \t NOUN \t nourishd\n",
            "else \t ADV \t else\n",
            "madness \t NOUN \t madness\n",
            "discreet \t ADJ \t discreet\n",
            "choking \t VERB \t choke\n",
            "gall \t NOUN \t gall\n",
            "preserving \t VERB \t preserve\n",
            "farewell \t INTJ \t farewell\n",
            "soft \t ADJ \t soft\n",
            "wrong \t ADJ \t wrong\n",
            "tut \t INTJ \t tut\n",
            "lost \t VERB \t lose\n",
            "groan \t VERB \t groan\n",
            "sadly \t ADV \t sadly\n",
            "bid \t VERB \t bid\n",
            "woman \t NOUN \t woman\n",
            "aimd \t VERB \t aimd\n",
            "near \t ADV \t near\n",
            "supposed \t VERB \t suppose\n",
            "loved \t VERB \t love\n",
            "markman \t NOUN \t markman\n",
            "fair \t ADJ \t fair\n",
            "soonest \t ADV \t soon\n",
            "hit \t VERB \t hit\n",
            "shell \t PROPN \t shell\n",
            "cupids \t NOUN \t cupid\n",
            "arrow \t NOUN \t arrow\n",
            "dians \t PROPN \t dians\n",
            "strong \t ADJ \t strong\n",
            "chastity \t NOUN \t chastity\n",
            "armd \t PROPN \t armd\n",
            "childish \t ADJ \t childish\n",
            "bow \t NOUN \t bow\n",
            "unharmd \t ADV \t unharmd\n",
            "siege \t NOUN \t siege\n",
            "terms \t NOUN \t term\n",
            "bide \t VERB \t bide\n",
            "encounter \t NOUN \t encounter\n",
            "assailing \t VERB \t assail\n",
            "ope \t VERB \t ope\n",
            "lap \t NOUN \t lap\n",
            "saintseducing \t VERB \t saintseduce\n",
            "rich \t ADJ \t rich\n",
            "dies \t VERB \t die\n",
            "store \t NOUN \t store\n",
            "sworn \t VERB \t swear\n",
            "chaste \t NOUN \t chaste\n",
            "sparing \t NOUN \t sparing\n",
            "huge \t ADJ \t huge\n",
            "waste \t NOUN \t waste\n",
            "starved \t VERB \t starve\n",
            "severity \t NOUN \t severity\n",
            "cuts \t VERB \t cut\n",
            "posterity \t NOUN \t posterity\n",
            "wise \t ADJ \t wise\n",
            "wisely \t ADV \t wisely\n",
            "merit \t NOUN \t merit\n",
            "bliss \t VERB \t bliss\n",
            "making \t VERB \t make\n",
            "despair \t NOUN \t despair\n",
            "forsworn \t ADJ \t forsworn\n",
            "vow \t NOUN \t vow\n",
            "dead \t ADJ \t dead\n",
            "ruled \t VERB \t rule\n",
            "forget \t VERB \t forget\n",
            "think \t VERB \t think\n",
            "teach \t VERB \t teach\n",
            "giving \t VERB \t give\n",
            "liberty \t NOUN \t liberty\n",
            "unto \t ADP \t unto\n",
            "examine \t VERB \t examine\n",
            "beauties \t NOUN \t beauty\n",
            "exquisite \t ADJ \t exquisite\n",
            "question \t NOUN \t question\n",
            "masks \t NOUN \t mask\n",
            "kiss \t VERB \t kiss\n",
            "ladies \t NOUN \t lady\n",
            "brows \t NOUN \t brow\n",
            "hide \t VERB \t hide\n",
            "strucken \t VERB \t strucken\n",
            "blind \t ADJ \t blind\n",
            "precious \t ADJ \t precious\n",
            "treasure \t NOUN \t treasure\n",
            "eyesight \t ADJ \t eyesight\n",
            "mistress \t NOUN \t mistress\n",
            "passing \t VERB \t pass\n",
            "note \t NOUN \t note\n",
            "passd \t VERB \t passd\n",
            "canst \t ADJ \t canst\n",
            "doctrine \t NOUN \t doctrine\n",
            "debt \t NOUN \t debt\n",
            "****************************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"************************* Proper names *************************\")\n",
        "proper_names = get_properNames(tokens_sw)\n",
        "print(proper_names,\"\\n\")\n",
        "print(\"We can see that we don't have all the proper names of the document. For example, <<Gregory>> and <<Sampson>> do not appear in this list.\")\n",
        "print(\"Moreover, in this list, some common nouns have been classified as proper nouns (as <<hell>> or <<sun>> for example).\\n\")\n",
        "\n",
        "# Try to find Gregory and Sampson in the list\n",
        "searched_words = [\"gregory\", \"sampson\"]\n",
        "for token in tokens_sw:\n",
        "  if str(token.text) in searched_words:\n",
        "    print(token.text, token.pos_, token.dep_)\n",
        "\n",
        "print(\"We can see that <<Gregory>> and <<Sampson>> have been classified as Nouns and not proper names.\")\n",
        "print(\"We can try to change them to see if any change is observed : we can try to write them with upper case letters.\\n\")\n",
        "\n",
        "# We modify our cleaned_corpus\n",
        "cleaned_corpus_2 = []\n",
        "for document in cleaned_corpus:\n",
        "  document_test = document.replace(\"gregory\",\"GREGORY\")\n",
        "  document_test = document_test.replace(\"sampson\",\"SAMPSON\")\n",
        "  cleaned_corpus_2.append(document_test)\n",
        "\n",
        "# Find the tokens of the corpus\n",
        "tokens_2 = get_tokens(cleaned_corpus_2)\n",
        "\n",
        "# Remove the stop words : we remove the default stop words list\n",
        "tokens_sw_2 = remove_stopWords(tokens_2)\n",
        "\n",
        "proper_names_2 = get_properNames(tokens_sw_2)\n",
        "print(proper_names_2,\"\\n\")\n",
        "\n",
        "print(\"By doing this change, we can see that <<Gregory>> is now classified as a proper name.\\n\")\n",
        "\n",
        "searched_words = [\"GREGORY\", \"SAMPSON\"]\n",
        "for token in tokens_sw_2:\n",
        "  if str(token.text) in searched_words:\n",
        "    print(token.text, token.pos_, token.dep_)\n",
        "\n",
        "print(\"Nevertheless, <<Sampson>> is still classified as a noun...\")\n",
        "print(\"****************************************************************\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLW8EcvsTxkW",
        "outputId": "e74159ef-83cf-410f-80e4-4b31c40c0c6e"
      },
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************* Proper names *************************\n",
            "[hadst, romeo, juliet, william, shakespeare, john, abraham, hell, villain, speak, nephew, madam, worshippd, sun, goodmorrow, cousin, misshapen, hast, shell, dians, armd] \n",
            "\n",
            "We can see that we don't have all the proper names of the document. For example, <<Gregory>> and <<Sampson>> do not appear in this list.\n",
            "Moreover, in this list, some common nouns have been classified as proper nouns (as <<hell>> or <<sun>> for example).\n",
            "\n",
            "gregory NOUN ROOT\n",
            "sampson NOUN ROOT\n",
            "We can see that <<Gregory>> and <<Sampson>> have been classified as Nouns and not proper names.\n",
            "We can try to change them to see if any change is observed : we can try to write them with upper case letters.\n",
            "\n",
            "The corpus contains 587 different tokens.\n",
            "Before removing stop words : 587 words\n",
            "After removing stop words : 523 words\n",
            "[GREGORY, hadst, romeo, juliet, william, shakespeare, john, abraham, hell, villain, speak, nephew, madam, worshippd, sun, goodmorrow, cousin, misshapen, hast, shell, dians, armd] \n",
            "\n",
            "By doing this change, we can see that <<Gregory>> is now classified as a proper name.\n",
            "\n",
            "GREGORY PROPN ROOT\n",
            "SAMPSON NOUN ROOT\n",
            "Nevertheless, <<Sampson>> is still classified as a noun...\n",
            "****************************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"************************* Text Mapping *************************\")\n",
        "document = cleaned_corpus_2[12]\n",
        "print(\"Document selected : \", document)\n",
        "\n",
        "displacy.render(nlp(document), style='dep', jupyter=True, options={'distance': 110})\n",
        "print(\"****************************************************************\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "qmL9ffuSFACC",
        "outputId": "4c63cd09-8460-4f77-947c-57223cf8c3c6"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************* Text Mapping *************************\n",
            "Document selected :  have fought with the men i will be cruel with the\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"748ac6757ca74ec483d1b25dd4a41308-0\" class=\"displacy\" width=\"1260\" height=\"302.0\" direction=\"ltr\" style=\"max-width: none; height: 302.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"212.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">have</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"212.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"160\">fought</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"160\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"212.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"270\">with</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"270\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"212.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"380\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"380\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"212.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"490\">men</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"490\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"212.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"600\">i</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"600\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"212.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"710\">will</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"710\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"212.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"820\">be</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"820\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"212.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"930\">cruel</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"930\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"212.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1040\">with</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1040\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"212.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1150\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1150\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-748ac6757ca74ec483d1b25dd4a41308-0-0\" stroke-width=\"2px\" d=\"M70,167.0 C70,112.0 150.0,112.0 150.0,167.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-748ac6757ca74ec483d1b25dd4a41308-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,169.0 L62,157.0 78,157.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-748ac6757ca74ec483d1b25dd4a41308-0-1\" stroke-width=\"2px\" d=\"M180,167.0 C180,2.0 820.0,2.0 820.0,167.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-748ac6757ca74ec483d1b25dd4a41308-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M180,169.0 L172,157.0 188,157.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-748ac6757ca74ec483d1b25dd4a41308-0-2\" stroke-width=\"2px\" d=\"M180,167.0 C180,112.0 260.0,112.0 260.0,167.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-748ac6757ca74ec483d1b25dd4a41308-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M260.0,169.0 L268.0,157.0 252.0,157.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-748ac6757ca74ec483d1b25dd4a41308-0-3\" stroke-width=\"2px\" d=\"M400,167.0 C400,112.0 480.0,112.0 480.0,167.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-748ac6757ca74ec483d1b25dd4a41308-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M400,169.0 L392,157.0 408,157.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-748ac6757ca74ec483d1b25dd4a41308-0-4\" stroke-width=\"2px\" d=\"M290,167.0 C290,57.0 485.0,57.0 485.0,167.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-748ac6757ca74ec483d1b25dd4a41308-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M485.0,169.0 L493.0,157.0 477.0,157.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-748ac6757ca74ec483d1b25dd4a41308-0-5\" stroke-width=\"2px\" d=\"M620,167.0 C620,57.0 815.0,57.0 815.0,167.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-748ac6757ca74ec483d1b25dd4a41308-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M620,169.0 L612,157.0 628,157.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-748ac6757ca74ec483d1b25dd4a41308-0-6\" stroke-width=\"2px\" d=\"M730,167.0 C730,112.0 810.0,112.0 810.0,167.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-748ac6757ca74ec483d1b25dd4a41308-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M730,169.0 L722,157.0 738,157.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-748ac6757ca74ec483d1b25dd4a41308-0-7\" stroke-width=\"2px\" d=\"M840,167.0 C840,112.0 920.0,112.0 920.0,167.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-748ac6757ca74ec483d1b25dd4a41308-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M920.0,169.0 L928.0,157.0 912.0,157.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-748ac6757ca74ec483d1b25dd4a41308-0-8\" stroke-width=\"2px\" d=\"M950,167.0 C950,112.0 1030.0,112.0 1030.0,167.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-748ac6757ca74ec483d1b25dd4a41308-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1030.0,169.0 L1038.0,157.0 1022.0,157.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-748ac6757ca74ec483d1b25dd4a41308-0-9\" stroke-width=\"2px\" d=\"M1060,167.0 C1060,112.0 1140.0,112.0 1140.0,167.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-748ac6757ca74ec483d1b25dd4a41308-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1140.0,169.0 L1148.0,157.0 1132.0,157.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"**************************** Nouns *****************************\")\n",
        "# Get the number of nouns in the corpus\n",
        "most_repeated = get_mostRepeated_noun(cleaned_corpus_2, tokens_sw_2)\n",
        "print(\"The most repeated noun in the corpus is <<\", most_repeated[0][0],\">>. This noun is repeated\", most_repeated[0][1],\"times.\\n\")\n",
        "print(\"This word is quite often used in this corpus. In fact, this corpus corresponds to the 1st scene of the 1st act of Romeo and Juliet.\")\n",
        "print(\"And, in this scene, <<chamber>> is mentioned : \")\n",
        "print(\"- when the 2 servants speak about the importance of the chamber, and their desire to defend their masters' honor.\")\n",
        "print(\"       --> Here, the chamber corresponds to a symbol of social status\")\n",
        "print(\"- during the argument between the servants\")\n",
        "print(\"       --> Here, chamber is used as its first meaning : a part of the house.\\n\")\n",
        "\n",
        "print(\"Hence, the chamber is mentioned as a symbol of social status, is associated to intimity and secrets, and is a potential place of conflicts\")\n",
        "print(\"All those remarks could explain why this word is such used in the corpus.\")\n",
        "print(\"****************************************************************\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qwNHv3RsAyG",
        "outputId": "2d517f62-6605-4375-8377-acddf180ad19"
      },
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************************** Nouns *****************************\n",
            "The corpus contains 213 nouns.\n",
            "The most repeated noun in the corpus is << chamber >>. This noun is repeated 12 times.\n",
            "\n",
            "This word is quite often used in this corpus. In fact, this corpus corresponds to the 1st scene of the 1st act of Romeo and Juliet.\n",
            "And, in this scene, <<chamber>> is mentioned : \n",
            "- when the 2 servants speak about the importance of the chamber, and their desire to defend their masters' honor.\n",
            "       --> Here, the chamber corresponds to a symbol of social status\n",
            "- during the argument between the servants\n",
            "       --> Here, chamber is used as its first meaning : a part of the house.\n",
            "\n",
            "Hence, the chamber is mentioned as a symbol of social status, is associated to intimity and secrets, and is a potential place of conflicts\n",
            "All those remarks could explain why this word is such used in the corpus.\n",
            "****************************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"************************** Root Words **************************\")\n",
        "print(\"Root words from corpus that have frequency higher than 15 :\")\n",
        "print(\"\\n 1 - By considering all the tokens (which could be repeated)\")\n",
        "print(more_frequent_roots(cleaned_corpus_2,15))\n",
        "print(\"\\n 2 - By considering unique tokens (non repeated)\")\n",
        "print(more_frequent_roots2(tokens_sw_2,15))\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"We can see that the first list is not really interesting to find useful information (most of them are prepositions or linkers).\")\n",
        "print(\"We can just deduce that Romeo and Benvolio are part of the main characters of the book.\")\n",
        "print(\"Also, because <<I>> and <<You>> are repeated quite often, we can deduce that there are some dialogs.\")\n",
        "print(\"Nevertheless, this piece of information could have been obtained, knowing the title of \")\n",
        "print(\"the book, and that it is a drama.\")\n",
        "print(\"\\n\")\n",
        "print(\"When we consider only unique tokens, we get more useful information. Certainly, the frequencies\")\n",
        "print(\"are well smaller, but the words obtained give us more information : this book is about Love, Sadness, \")\n",
        "print(\"conflict; with Romeo as main character.\")\n",
        "print(\"\\n\")\n",
        "print(\"But, by combining those 2 groups of words, we can find complementary information.\")\n",
        "print(\"****************************************************************\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HLHu71nLzzA",
        "outputId": "c9e81a38-0276-44f5-adc4-6cbe96b768fd"
      },
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************** Root Words **************************\n",
            "Root words from corpus that have frequency higher than 15 :\n",
            "\n",
            " 1 - By considering all the tokens (which could be repeated)\n",
            "           0\n",
            "the       58\n",
            "and       53\n",
            "i         44\n",
            "of        39\n",
            "to        37\n",
            "romeo     32\n",
            "in        28\n",
            "you       27\n",
            "benvolio  27\n",
            "that      26\n",
            "a         24\n",
            "is        24\n",
            "          24\n",
            "me        23\n",
            "by        23\n",
            "\n",
            " 2 - By considering unique tokens (non repeated)\n",
            "        0\n",
            "love    4\n",
            "sad     3\n",
            "thrust  2\n",
            "cut     2\n",
            "romeo   2\n",
            "hous    2\n",
            "weapon  2\n",
            "pass    2\n",
            "blow    2\n",
            "part    2\n",
            "sever   2\n",
            "ladi    2\n",
            "give    2\n",
            "brawl   2\n",
            "glad    2\n",
            "\n",
            "\n",
            "We can see that the first list is not really interesting to find useful information (most of them are prepositions or linkers).\n",
            "We can just deduce that Romeo and Benvolio are part of the main characters of the book.\n",
            "Also, because <<I>> and <<You>> are repeated quite often, we can deduce that there are some dialogs.\n",
            "Nevertheless, this piece of information could have been obtained, knowing the title of \n",
            "the book, and that it is a drama.\n",
            "\n",
            "\n",
            "When we consider only unique tokens, we get more useful information. Certainly, the frequencies\n",
            "are well smaller, but the words obtained give us more information : this book is about Love, Sadness, \n",
            "conflict; with Romeo as main character.\n",
            "\n",
            "\n",
            "But, by combining those 2 groups of words, we can find complementary information.\n",
            "****************************************************************\n",
            "\n"
          ]
        }
      ]
    }
  ]
}